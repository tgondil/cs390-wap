# Class 5: Scaling Modern Applications - From Startup to Unicorn

**Duration:** 40 minutes  
**Topic:** Scaling Data, Services, and Performance for High-Growth Applications

---

## üéØ Learning Objectives

By the end of this class, students will understand:
- Why scaling is critical for business survival and how it directly impacts revenue
- How to scale data storage through replication, sharding, and specialized storage solutions
- When and how to transition from monoliths to microservices
- Performance optimization strategies that can reduce infrastructure costs by 70%+
- Real-world scaling decisions that made or broke billion-dollar companies

---

## 1. üöÄ The Growth Dilemma: Why Scale or Die

### The WhatsApp Miracle: 55 Employees, 900 Million Users

In 2014, Facebook acquired WhatsApp for $19 billion. WhatsApp was serving **900 million users** with just **55 employees**. How?

**The Scaling Secret:**
- **Simple architecture** that could scale horizontally
- **Erlang-based backend** designed for massive concurrency
- **Minimal features** but rock-solid performance
- **Strategic scaling decisions** made early

**Compare to traditional telecom:**
- Verizon: ~195,000 employees for ~120 million customers
- WhatsApp: ~55 employees for ~900 million users
- **That's 300x more efficient per employee**

### The Twitter Fail Whale Era: When Scaling Goes Wrong

**2008-2010: Twitter's Dark Years**
- Ruby on Rails monolith couldn't handle viral moments
- **"Fail Whale" appeared during every major event**
- Lost users to Facebook during critical growth period
- Stock price languished until scaling problems were solved

**The Turnaround (2010-2012):**
- Migrated from Rails monolith to Scala microservices
- Implemented proper caching layers
- Built custom message queues
- **Result:** Handled 500,000 tweets per second during 2014 World Cup

**Business Impact:**
- Pre-scaling: Frequent downtime, user frustration, slow growth
- Post-scaling: Became the real-time news platform, IPO success

---

## 2. üóÉÔ∏è Scaling Data: From Single Database to Global Infrastructure

### The Problem: When Your Database Becomes the Bottleneck

**Instagram's 2010 Crisis:**
When Instagram launched, they had a single PostgreSQL database. Within weeks:
- **Read queries taking 2+ seconds**
- **Write operations timing out**
- **Photo uploads failing during peak hours**
- **Users abandoning the app**

### Master-Slave Replication: The First Line of Defense

**Why Replication Exists:**
Most applications are **read-heavy**. Think about it:
- **Instagram:** You scroll through hundreds of photos but only post a few
- **YouTube:** Millions watch videos, thousands upload
- **Twitter:** Everyone reads the timeline, fewer people tweet

**Master-Slave Architecture:**
```
                    ‚îå‚îÄ‚îÄ‚îÄ Slave 1 (US-East) ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    Read-only             ‚îÇ
Master (US-West) ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ Slave 2 (Europe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ Load Balancer
Write operations    ‚îÇ    Read-only             ‚îÇ    (Read queries)
                    ‚îî‚îÄ‚îÄ‚îÄ Slave 3 (Asia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         Read-only
```

**Real Implementation: Spotify's Music Streaming**

When you play a song on Spotify:
- **Write operation (rare):** Adding song to playlist ‚Üí Goes to Master in Sweden
- **Read operations (constant):** 
  - Loading your playlists ‚Üí Nearest slave (US, Europe, Asia)
  - Searching for songs ‚Üí Nearest slave with search index
  - Getting song metadata ‚Üí Local slave

**Benefits:**
- **Performance:** Reads are lightning fast (local slaves)
- **Reliability:** If master fails, promote a slave
- **Geographic Distribution:** Users hit local databases

**The Trade-off:**
- **Eventual Consistency:** New playlist might not appear immediately on all devices
- **Complexity:** Managing replication lag and failover scenarios

### Multi-Master Replication: When Every Microsecond Counts

**When Master-Slave Isn't Enough:**

**Figma's Real-Time Collaboration Challenge:**
Multiple designers editing the same file simultaneously from different continents.

**The Problem with Single Master:**
- Designer in Tokyo makes a change
- Must travel to Master in US ‚Üí **150ms latency**
- Change propagates back to London designer ‚Üí **another 150ms**
- **Total delay: 300ms** (feels laggy and broken)

**Multi-Master Solution:**
```
Master (US) ‚Üê‚Üí Master (Europe) ‚Üê‚Üí Master (Asia)
     ‚Üë              ‚Üë                ‚Üë
   US users    European users    Asian users
```

**How Conflict Resolution Works:**
```javascript
// Figma's conflict resolution example
user_tokyo_action = {
  timestamp: "2024-01-15T10:30:00.123Z",
  user_id: "tokyo_designer",
  action: "move_rectangle",
  object_id: "rect_123",
  new_position: {x: 150, y: 200}
}

user_london_action = {
  timestamp: "2024-01-15T10:30:00.127Z", // 4ms later
  user_id: "london_designer", 
  action: "resize_rectangle",
  object_id: "rect_123",
  new_size: {width: 100, height: 50}
}

// System applies both changes in timestamp order
// Final result: Rectangle is moved AND resized
```

**Trade-offs:**
- ‚úÖ **Ultra-low latency** for global users
- ‚úÖ **High availability** (any master can fail)
- ‚ùå **Complex conflict resolution**
- ‚ùå **Eventual consistency challenges**

### Sharding: Breaking the Single Database Limit

**Why Shard?**
Even with replication, you eventually hit the **write bottleneck**. One master can only handle so many concurrent writes before it melts down.

**Discord's Massive Scale Problem (2019):**
- **150 million monthly users**
- **Billions of messages per day**
- Single database could only handle ~10,000 writes/second
- Needed to support **100,000+ writes/second during major events**

#### Range-Based Sharding: Organizing by Natural Boundaries

**Discord's Server-Based Sharding:**
```
Shard 1: Server IDs 1-100,000
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gaming servers (US-West)        ‚îÇ
‚îÇ - Fortnite communities          ‚îÇ
‚îÇ - Call of Duty clans            ‚îÇ
‚îÇ - Minecraft servers             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Shard 2: Server IDs 100,001-200,000  
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Study groups (Global)           ‚îÇ
‚îÇ - University servers            ‚îÇ
‚îÇ - Language learning             ‚îÇ
‚îÇ - Programming communities       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Shard 3: Server IDs 200,001-300,000
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ International communities       ‚îÇ 
‚îÇ - K-pop fandoms                 ‚îÇ
‚îÇ - Anime discussions             ‚îÇ
‚îÇ - Regional servers              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Benefits:**
- **Logical organization:** Related data stays together
- **Predictable queries:** Finding a server's messages is straightforward
- **Easy range expansion:** Just adjust the boundaries

**The Problem:**
- **Hotspots:** What if Shard 1 has all the active gaming communities?
- **Uneven growth:** Gaming servers might grow 10x faster than study groups

#### Hash-Based Sharding: Even Distribution

**Instagram's Photo Storage Sharding:**
```javascript
// Instagram's sharding algorithm
function getPhotoShard(photo_id) {
  // Use MD5 hash for even distribution
  const hash = md5(photo_id);
  const shard_number = hash % NUMBER_OF_SHARDS;
  return shard_number;
}

// Example:
photo_id = "IMG_2024_beach_sunset_456789"
hash = md5(photo_id) = "a1b2c3d4e5f6..." 
shard = hash % 100 = 73
// Photo goes to Shard 73
```

**Even Distribution:**
```
Each shard gets ~1% of photos randomly distributed

Shard 1: Random mix of all types of photos (1%)
Shard 2: Random mix of all types of photos (1%)
...
Shard 100: Random mix of all types of photos (1%)
```

**Benefits:**
- **Perfect load distribution:** Each shard gets equal load
- **No hotspots:** Viral photos are distributed randomly
- **Predictable performance:** Each shard performs identically

**The Trade-off:**
- **Cross-shard queries are expensive:** "Show me all photos from this user" might require checking all 100 shards

#### Geographic Sharding: Optimizing for Global Users

**Uber's City-Based Sharding:**
```
San Francisco Shard (US-West)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ All SF rides, drivers, users    ‚îÇ
‚îÇ - Rides within SF city limits   ‚îÇ
‚îÇ - SF driver locations           ‚îÇ
‚îÇ - SF user profiles              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

New York Shard (US-East)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ All NYC rides, drivers, users   ‚îÇ
‚îÇ - Manhattan rides               ‚îÇ
‚îÇ - Brooklyn driver locations     ‚îÇ
‚îÇ - NYC user profiles             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

London Shard (Europe)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ All London rides, drivers, users‚îÇ
‚îÇ - Central London rides          ‚îÇ
‚îÇ - UK driver locations           ‚îÇ
‚îÇ - European user profiles        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Why This Works for Uber:**
- **99.9% of queries are local:** Finding nearby drivers in SF doesn't need NYC data
- **Low latency:** SF users hit SF database (same data center)
- **Regulatory compliance:** European data stays in Europe (GDPR)

**The Challenge:**
- **Cross-city trips:** SF to Oakland might cross shard boundaries
- **Business analytics:** "Global revenue report" requires querying all shards

### Indexing: Making Shards Lightning Fast

**The Problem Within Shards:**
Even with perfect sharding, individual shards can still be slow without proper indexing.

**Slack's Message Search Challenge:**
```sql
-- Without index: Scans 50 million messages (5+ seconds)
SELECT * FROM messages 
WHERE channel_id = 'general' 
AND created_at > '2024-01-01' 
AND content LIKE '%urgent%';

-- With composite index: Returns in 50ms
CREATE INDEX idx_channel_time_search 
ON messages (channel_id, created_at, content);
```

**Strategic Index Design:**

**B-Tree Indexes for Range Queries:**
```sql
-- Perfect for: "Show messages from last 24 hours"
CREATE INDEX idx_messages_time ON messages (created_at);

-- Perfect for: "All messages in this channel, newest first"  
CREATE INDEX idx_channel_time ON messages (channel_id, created_at DESC);
```

**Partial Indexes for Common Patterns:**
```sql
-- Only index unread messages (much smaller index)
CREATE INDEX idx_unread_messages 
ON messages (user_id, created_at) 
WHERE read_status = 'unread';

-- Only index recent messages (rolling 30-day window)
CREATE INDEX idx_recent_messages 
ON messages (channel_id, created_at)
WHERE created_at > NOW() - INTERVAL '30 days';
```

**The Index Trade-off:**
- ‚úÖ **Lightning-fast reads:** 1000x faster queries
- ‚ùå **Slower writes:** Every insert must update indexes
- ‚ùå **Storage overhead:** Indexes can be 30-50% of data size

### Blob/Object Storage: When Your Database Isn't the Right Tool

**YouTube's Storage Architecture Evolution:**

**Early YouTube (2005):** Everything in MySQL
```sql
videos_table:
- video_id
- title  
- description
- video_file_data (BLOB) ‚Üê This was the problem!
```

**The Problem:**
- **Single 1GB video = 1GB in database**
- **Database backups included video files**
- **Queries slow down due to massive row sizes**
- **Cannot distribute video files geographically**

**Modern YouTube Architecture:**
```sql
videos_table:
- video_id
- title
- description  
- s3_video_url ‚Üê Just a reference!

-- Actual video files stored in:
s3://youtube-videos-us-west/abc123.mp4
s3://youtube-videos-europe/abc123.mp4  
s3://youtube-videos-asia/abc123.mp4
```

**Netflix's Global CDN Strategy:**
```
User in Tokyo requests "Stranger Things S4E1"

1. Netflix app checks: videos_table in Tokyo database
   - Gets metadata: title, duration, thumbnail_url
   - Gets video reference: s3://netflix-asia/stranger-things-s4e1.mp4

2. Video player requests actual video file
   - Routed to nearest CDN edge server in Tokyo
   - If not cached locally, fetches from S3 Asia
   - Streams directly to user's device

Result: 
- Database query: 5ms (local database)  
- Video streaming: 50ms (local CDN)
- No database involved in actual video delivery
```

**When to Use Object Storage:**
- **Large files:** Images, videos, documents, backups
- **Static content:** User avatars, product photos, PDFs
- **Infrequent access:** Archive data, old files
- **Global distribution:** Content served worldwide

**When to Keep in Database:**
- **Small data:** User profiles, settings, preferences
- **Frequent queries:** Session data, shopping carts
- **Transactional data:** Orders, payments, user actions
- **Complex relationships:** Data that needs JOINs

---

## 3. üèóÔ∏è Scaling Services: From Monolith to Microservices

### The Monolith: When Simple is Better

**Basecamp's $100M Monolith Success:**
Basecamp serves millions of users with a **single Rails monolith**. They've resisted microservices for 20+ years.

**Why Their Monolith Works:**
```ruby
# Single codebase handles everything:
class ProjectsController
  def create
    @project = Project.create(params)
    NotificationService.send_email(@project)  # Same process
    AnalyticsService.track("project_created") # Same database
    BillingService.update_usage(@project)     # Same transaction
  end
end
```

**Monolith Advantages:**
- **Single deployment:** One Git push deploys everything
- **Shared database:** ACID transactions across all features
- **Simple debugging:** Everything in one place
- **Easy development:** No network calls between features
- **Consistent performance:** No inter-service latency

**When Monoliths Make Sense:**
- **Small team** (< 10 developers)
- **Well-defined domain** (project management, e-commerce)
- **Consistent technology stack**
- **Predictable scaling requirements**

### The Breaking Point: When Monoliths Become Liabilities

**Shopify's Monolith Crisis (2014-2016):**

**The Problem:**
```ruby
# Single monolith handling everything:
- Product catalog (millions of products)
- Order processing (Black Friday traffic spikes)  
- Payment processing (PCI compliance requirements)
- Inventory management (real-time stock updates)
- Email marketing (bulk sending)
- Analytics (heavy reporting queries)
```

**What Broke:**
- **Deploy paralysis:** Any code change required full system deployment
- **Performance conflicts:** Analytics queries slowed down checkout
- **Team conflicts:** 200+ developers stepping on each other
- **Scaling limits:** Couldn't scale catalog without scaling payments
- **Risk amplification:** Bug in analytics could crash checkout

### Microservices: The Divide and Conquer Approach

**Shopify's Microservices Solution:**
```
                        API Gateway
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                    ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇProducts‚îÇ         ‚îÇ Orders  ‚îÇ         ‚îÇ Payments  ‚îÇ
    ‚îÇService ‚îÇ         ‚îÇ Service ‚îÇ         ‚îÇ Service   ‚îÇ
    ‚îÇ        ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ           ‚îÇ
    ‚îÇ- Search‚îÇ         ‚îÇ- Cart   ‚îÇ         ‚îÇ- Stripe   ‚îÇ
    ‚îÇ- Images‚îÇ         ‚îÇ- Checkout‚îÇ        ‚îÇ- PayPal   ‚îÇ
    ‚îÇ- Prices‚îÇ         ‚îÇ- History‚îÇ         ‚îÇ- Security ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                    ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇProducts ‚îÇ         ‚îÇ Orders  ‚îÇ         ‚îÇ Payments  ‚îÇ
   ‚îÇDatabase ‚îÇ         ‚îÇDatabase ‚îÇ         ‚îÇ Database  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Service Boundaries by Business Domain:**

**Products Service:**
```javascript
// Independent scaling and development
class ProductsService {
  // Heavy read traffic during browsing
  async searchProducts(query) {
    // Can scale independently with read replicas
    return await ProductSearch.find(query);
  }
  
  // Infrequent write traffic
  async updateProduct(id, changes) {
    return await Product.update(id, changes);
  }
}
```

**Orders Service:**
```javascript  
// Traffic spikes during sales events
class OrdersService {
  // Massive traffic during Black Friday
  async createOrder(cart) {
    // Can scale horizontally just for order processing
    return await Order.create(cart);
  }
  
  // Calls other services
  async processOrder(orderId) {
    await PaymentsService.charge(orderId);
    await InventoryService.reserveItems(orderId);
    await EmailService.sendConfirmation(orderId);
  }
}
```

**Benefits Realized:**
- **Independent scaling:** Scale orders service 10x during Black Friday, keep products service normal
- **Team autonomy:** Products team deploys without affecting orders team
- **Technology diversity:** Use best tool for each job (search engine for products, queue system for orders)
- **Fault isolation:** Bug in products doesn't crash checkout

### The Microservices Tax: Hidden Costs

**The Network is Not Reliable:**

**Before (Monolith):**
```javascript
// Function call within same process (1ms)
function processOrder(cart) {
  const user = UserService.getUser(cart.userId);        // 1ms
  const inventory = InventoryService.check(cart.items); // 1ms  
  const payment = PaymentService.charge(cart.total);    // 1ms
  return new Order(user, inventory, payment);
}
// Total time: ~3ms
```

**After (Microservices):**
```javascript
// Network calls between services
async function processOrder(cart) {
  const user = await UserService.getUser(cart.userId);        // 50ms + network
  const inventory = await InventoryService.check(cart.items); // 50ms + network
  const payment = await PaymentService.charge(cart.total);    // 50ms + network
  return new Order(user, inventory, payment);
}
// Total time: ~150ms + failure risk
```

**Cascading Failures:**
```
User tries to checkout:
1. Orders Service (UP) calls Inventory Service (DOWN)
2. Inventory Service timeout after 30 seconds
3. User sees "Checkout Failed" after waiting 30 seconds
4. Order is half-processed (payment charged but no inventory reserved)
```

**Distributed Debugging Hell:**
```
Bug report: "Order #12345 failed"

Investigation requires checking:
- Orders Service logs (which server?)
- Inventory Service logs (which instance?)
- Payment Service logs (which region?)
- API Gateway logs (which route?)
- Database logs (which shard?)
- Network logs (which load balancer?)

Single bug requires 6 different log searches across multiple teams
```

### When to Choose Each Approach

**Stay with Monolith when:**
- **Team size:** < 10 developers
- **Deployment frequency:** < 1x per day
- **Performance requirements:** < 100ms latency critical
- **Data consistency:** Strong ACID requirements
- **Operational complexity:** Limited DevOps resources

**Move to Microservices when:**
- **Team size:** > 20 developers across multiple teams
- **Independent scaling:** Different parts have wildly different load patterns
- **Technology diversity:** Need different languages/databases for different problems
- **Deployment independence:** Teams need to deploy independently
- **Fault isolation:** Cannot afford for one feature to bring down others

---

## 4. ‚ö° Load Balancing: Distributing the Traffic

### Round Robin: The Simple Solution

**How It Works:**
```
Request 1 ‚Üí Server A
Request 2 ‚Üí Server B  
Request 3 ‚Üí Server C
Request 4 ‚Üí Server A (back to start)
```

**Good for:**
- **Stateless applications:** Each server can handle any request
- **Uniform server capacity:** All servers have identical specs
- **Simple applications:** No user sessions or sticky state

**Reddit's Comment System:**
```javascript
// Any server can handle any comment request
app.get('/api/comments/:postId', (req, res) => {
  // No user state needed, just fetch comments from database
  const comments = await Comments.find({postId: req.params.postId});
  res.json(comments);
});

// Round robin works perfectly - no state to maintain
```

### Least Connections: Accounting for Reality

**The Problem with Round Robin:**
```
Server A: Currently processing 10 long-running requests (heavy video uploads)
Server B: Currently processing 2 quick requests (simple API calls)
Server C: Currently processing 15 file downloads

Round Robin would send next request to Server A (the busiest one!)
```

**Least Connections Solution:**
```
Current state:
- Server A: 10 active connections
- Server B: 2 active connections  ‚Üê Send here!
- Server C: 15 active connections

Load balancer always sends to server with fewest active connections
```

**Zoom's Video Processing:**
```javascript
// Video processing servers have varying loads
Server A: Processing 3 HD video calls (CPU intensive)
Server B: Processing 12 audio-only calls (light load)  
Server C: Processing 1 4K video call (very heavy)

Least connections algorithm:
- Counts actual processing load, not just request count
- Routes new video call to Server B (lightest actual load)
```

### Consistent Hashing: Sticky Sessions Done Right

**The Problem:**
E-commerce shopping carts stored in server memory (not database):

```javascript
// Shopping cart stored in server RAM
let userCarts = new Map();

app.post('/cart/add', (req, res) => {
  const userId = req.session.userId;
  userCarts.set(userId, [...userCarts.get(userId), req.body.item]);
});

// Problem: If user gets routed to different server, cart is empty!
```

**Bad Solution: Sticky Sessions**
```
Always route User A to Server 1
Always route User B to Server 2  
Always route User C to Server 3

Problems:
- Server 1 fails ‚Üí User A loses their cart completely
- Server 1 gets all the heavy users ‚Üí uneven load
- Adding new servers ‚Üí existing sessions break
```

**Consistent Hashing Solution:**
```javascript
// Hash user ID to determine server
function getServerForUser(userId) {
  const hash = sha256(userId);
  const serverIndex = hash % servers.length;
  return servers[serverIndex];
}

// Virtual nodes for better distribution
function getServerWithVirtualNodes(userId) {
  const hash = sha256(userId);
  // 100 virtual nodes per physical server
  const virtualNode = hash % (servers.length * 100);
  const physicalServer = Math.floor(virtualNode / 100);
  return servers[physicalServer];
}
```

**Adding Servers Gracefully:**
```
Before: 3 servers, each handling 33% of users
Server A: Users 1-33
Server B: Users 34-66  
Server C: Users 67-100

Adding Server D:
Server A: Users 1-25     (moved 8 users to D)
Server B: Users 26-50    (moved 8 users to D)
Server C: Users 51-75    (moved 8 users to D)
Server D: Users 76-100   (got 24 users total)

Only 24% of users need to move sessions, not 100%!
```

**Twitch's Streaming Consistency:**
```javascript
// Each streamer always hits same server for their stream
function getStreamServer(streamerId) {
  // Consistent hashing ensures same streamer ‚Üí same server
  return consistentHash(streamerId);
}

// Benefits:
// - Stream state stays consistent
// - Chat moderator actions persist
// - Viewer count accurate
// - Adding servers doesn't break ongoing streams
```

---

## 5. üöÄ Performance: Caching and Content Delivery

### The 80/20 Rule of Web Performance

**Reddit's Data Pattern:**
- **80% of requests:** Viewing popular posts (read-heavy)
- **20% of requests:** Creating posts, voting, commenting (write-heavy)

**The Insight:** Cache the 80% to make everything faster.

### Redis: The Speed Demon

**How Redis Achieves Microsecond Performance:**

**In-Memory Storage:**
```
Traditional Database (Disk):
Request ‚Üí SQL Query ‚Üí Disk Seek (5-10ms) ‚Üí Result
Total: ~10ms per query

Redis (Memory):
Request ‚Üí Hash Lookup ‚Üí RAM Access (0.1ms) ‚Üí Result  
Total: ~0.1ms per query

Redis is 100x faster for simple lookups!
```

**Real-World Example: Instagram's Feed Generation**

**Without Redis (slow):**
```javascript
// Every time user opens Instagram
app.get('/feed/:userId', async (req, res) => {
  // Query database for user's followers (50ms)
  const followers = await User.find({following: userId});
  
  // Query database for recent posts from all followers (200ms)
  const posts = await Post.find({
    userId: {$in: followers}, 
    createdAt: {$gt: Date.now() - 24*60*60*1000}
  }).sort({createdAt: -1}).limit(50);
  
  // Query database for engagement data (100ms)
  const postsWithEngagement = await Promise.all(
    posts.map(async post => ({
      ...post,
      likes: await Like.count({postId: post.id}),
      comments: await Comment.count({postId: post.id})
    }))
  );
  
  res.json(postsWithEngagement);
});

// Total time: 350ms for popular user with many followers
```

**With Redis (fast):**
```javascript
app.get('/feed/:userId', async (req, res) => {
  // Check Redis cache first (1ms)
  const cachedFeed = await redis.get(`feed:${userId}`);
  if (cachedFeed) {
    return res.json(JSON.parse(cachedFeed)); // 1ms total!
  }
  
  // Cache miss - generate feed and cache it
  const feed = await generateFeedFromDatabase(userId); // 350ms
  await redis.setex(`feed:${userId}`, 300, JSON.stringify(feed)); // Cache for 5 minutes
  
  res.json(feed);
});

// First request: 350ms (cache miss)
// Next 100 requests in 5 minutes: 1ms each (cache hit)
// Average response time drops from 350ms to ~4ms
```

**Cache Patterns in Action:**

**Write-Through (Strong Consistency):**
```javascript
// Every write updates both database and cache
async function updateUserProfile(userId, changes) {
  // Update database first
  const updatedUser = await User.update(userId, changes);
  
  // Update cache immediately  
  await redis.set(`user:${userId}`, JSON.stringify(updatedUser));
  
  return updatedUser;
}

// Pro: Cache always matches database
// Con: Slower writes (two operations)
// Good for: Critical data like user profiles, account balances
```

**Write-Back (Performance First):**
```javascript
// Write to cache immediately, database later
async function updateUserPreferences(userId, preferences) {
  // Update cache immediately (fast response to user)
  await redis.set(`preferences:${userId}`, JSON.stringify(preferences));
  
  // Queue database update for later (async)
  await backgroundQueue.add('updateUserPreferences', {userId, preferences});
  
  return preferences;
}

// Pro: Super fast writes
// Con: Risk of data loss if cache fails before database update
// Good for: Non-critical data like UI preferences, themes
```

**Read-Through (Automatic Cache Population):**
```javascript
async function getUserProfile(userId) {
  // Check cache first
  let user = await redis.get(`user:${userId}`);
  
  if (!user) {
    // Cache miss - fetch from database and populate cache
    user = await User.findById(userId);
    await redis.setex(`user:${userId}`, 3600, JSON.stringify(user)); // 1 hour TTL
  } else {
    user = JSON.parse(user);
  }
  
  return user;
}

// Pro: Cache automatically stays populated with requested data
// Con: First request is slow (cache miss penalty)
// Good for: User profiles, product details
```

### Cache Invalidation: The Two Hard Problems

> "There are only two hard things in Computer Science: cache invalidation and naming things." - Phil Karlton

**The Problem:**
```javascript
// User updates their profile photo
await User.update(userId, {avatar: 'new-photo.jpg'});

// But cache still has old photo!
const cachedUser = await redis.get(`user:${userId}`); 
// Returns: {avatar: 'old-photo.jpg'} ‚Üê Wrong!
```

**Solution 1: Time-Based Expiration (TTL)**
```javascript
// Cache expires automatically after 1 hour
await redis.setex(`user:${userId}`, 3600, JSON.stringify(user));

// Pro: Simple, prevents stale data from lasting forever
// Con: Users might see old data for up to 1 hour
// Good for: Data that changes infrequently
```

**Solution 2: Event-Based Invalidation**
```javascript
async function updateUserProfile(userId, changes) {
  // Update database
  const updatedUser = await User.update(userId, changes);
  
  // Invalidate all related caches
  await redis.del(`user:${userId}`);
  await redis.del(`user:${userId}:feed`);
  await redis.del(`user:${userId}:followers`);
  
  return updatedUser;
}

// Pro: Always fresh data
// Con: Complex to track all related cache keys
// Good for: Critical data like prices, inventory
```

**Solution 3: Versioned Caching**
```javascript
// Include version number in cache key
const version = await redis.incr(`user:${userId}:version`);
await redis.set(`user:${userId}:v${version}`, JSON.stringify(user));

// Reading always uses latest version
async function getUser(userId) {
  const version = await redis.get(`user:${userId}:version`);
  return await redis.get(`user:${userId}:v${version}`);
}

// Pro: No cache invalidation needed, atomic updates
// Con: More complex, uses more memory
// Good for: High-frequency updates
```

### CDN: Bringing Content Closer to Users

**The Physics Problem:**
```
User in Tokyo requests image from server in New York:
- Distance: ~6,700 miles
- Speed of light: 186,000 miles/second
- Theoretical minimum latency: 36ms (one way)
- Real internet latency: 150-200ms (due to routing, processing)
```

**CDN Solution:**
```
Without CDN:
Tokyo User ‚Üí New York Server (200ms)

With CDN:
Tokyo User ‚Üí Tokyo CDN Edge (5ms)
If cache miss: Tokyo CDN ‚Üí New York Server (200ms) ‚Üí Tokyo CDN ‚Üí User (5ms)
```

**Netflix's Global CDN Architecture:**

**Content Strategy:**
```javascript
// Popular content (viewed by millions) - cached everywhere
const popularContent = [
  'stranger-things-s4',
  'wednesday-series', 
  'squid-game'
];

// Long-tail content (viewed by thousands) - cached regionally  
const regionalContent = [
  'korean-drama-series',
  'japanese-anime-movie',
  'indian-bollywood-film'
];

// Fresh content (just released) - origin server only
const freshContent = [
  'newly-uploaded-today'
];
```

**Cache Hierarchy:**
```
Level 1: Edge Servers (1000+ locations worldwide)
‚îú‚îÄ‚îÄ Hot content: Top 1% of videos (instant access)
‚îî‚îÄ‚îÄ Cache duration: 24 hours

Level 2: Regional Servers (50+ major cities)  
‚îú‚îÄ‚îÄ Popular content: Top 20% of videos  
‚îî‚îÄ‚îÄ Cache duration: 7 days

Level 3: Origin Servers (3-5 global locations)
‚îú‚îÄ‚îÄ All content: 100% of Netflix library
‚îî‚îÄ‚îÄ Permanent storage
```

**Real Performance Impact:**
```
Content Request Performance:

Cache Hit at Edge (99% of requests):
- Latency: 10-50ms
- Bandwidth: Full speed (no internet bottleneck)
- User experience: Instant streaming

Cache Miss to Regional (0.9% of requests):
- Latency: 50-100ms  
- Still fast enough for good experience

Cache Miss to Origin (0.1% of requests):
- Latency: 200ms+
- Only happens for very new/rare content
```

### The Performance Stack: How It All Works Together

**Spotify's Full Performance Architecture:**

**Layer 1: CDN (Static Assets)**
```
song-artwork.jpg, album-covers.png ‚Üí CloudFlare CDN
- Cached globally for 30 days
- 50ms average load time worldwide
```

**Layer 2: Redis (Hot Data)**  
```
user-playlists, recently-played, search-results ‚Üí Redis
- Top 10% most accessed data
- 1ms response time
- 5-minute cache duration
```

**Layer 3: Database Read Replicas (Warm Data)**
```
user-profiles, song-metadata ‚Üí PostgreSQL slaves
- Regional replicas for faster access
- 50ms response time
- Always up-to-date
```

**Layer 4: Primary Database (Cold Data)**
```
payment-history, account-settings ‚Üí PostgreSQL master
- Accessed infrequently
- 100ms response time
- Authoritative source of truth
```

**Request Flow Example:**
```
User searches for "Taylor Swift":

1. Check Redis cache (1ms) ‚Üí Cache hit!
   Returns: Top 10 Taylor Swift songs

2. Load album artwork (10ms) ‚Üí CDN hit!  
   Returns: High-res images from nearest edge server

3. Load song previews (15ms) ‚Üí CDN hit!
   Returns: 30-second audio clips from regional server

Total response time: 26ms
Without caching: Would be 500ms+ (database + asset loading)
```

---

## 6. üéØ Wrap-Up: Scaling Decision Framework

### The Scaling Hierarchy: What to Optimize First

**1. Measure Before You Scale (Week 1)**
```javascript
// Add performance monitoring to everything
const startTime = Date.now();
const result = await someOperation();
const duration = Date.now() - startTime;

console.log(`Operation took ${duration}ms`);
```

**The 10x Rule:** Don't optimize until you have 10x more load than you can currently handle.

**2. Optimize the Database (Week 2-3)**
- Add indexes to slow queries (100x speed improvement)
- Add read replicas for read-heavy workloads  
- Cache frequently accessed data in Redis

**3. Scale Horizontally (Week 4-6)**
- Add load balancers and multiple app servers
- Implement proper session management
- Monitor for bottlenecks

**4. Break Apart When Necessary (Month 2-3)**
- Extract microservices only when teams are blocked
- Start with the most independent features
- Maintain shared databases as long as possible

### Real-World Scaling Timeline

**Slack's Scaling Journey (2013-2020):**

**Year 1 (10K users):** Single Rails monolith + PostgreSQL
- **Team size:** 5 engineers
- **Infrastructure cost:** $500/month
- **Performance:** 200ms average response time

**Year 2 (100K users):** Added Redis cache + read replicas  
- **Team size:** 15 engineers
- **Infrastructure cost:** $5,000/month  
- **Performance:** 100ms average response time

**Year 3 (1M users):** Multiple app servers + load balancer
- **Team size:** 30 engineers
- **Infrastructure cost:** $50,000/month
- **Performance:** 150ms average response time (scaling challenges)

**Year 4 (10M users):** First microservice extraction (real-time messaging)
- **Team size:** 100 engineers across 8 teams
- **Infrastructure cost:** $500,000/month
- **Performance:** 50ms average response time

**Year 7 (12M+ users):** Full microservices architecture
- **Team size:** 300+ engineers across 30+ teams  
- **Infrastructure cost:** $2M+/month
- **Performance:** 30ms average response time

### Key Takeaways

**Scaling is a Business Problem, Not Just Technical:**
- Instagram's 13-person team sold for $1B because they scaled smart
- Twitter nearly died because they scaled poorly
- Basecamp thrives with a monolith because it fits their business

**The Right Tool for the Right Job:**
- **SQL databases:** When you need ACID transactions and complex relationships
- **NoSQL databases:** When you need massive scale and flexible schemas  
- **Redis cache:** When you need microsecond performance for hot data
- **CDN:** When you need global content delivery
- **Microservices:** When you have multiple teams building different features

**Optimization Order Matters:**
1. **Add indexes** (hours of work, 100x performance gain)
2. **Add caching** (days of work, 10x performance gain)  
3. **Add read replicas** (weeks of work, 5x performance gain)
4. **Shard databases** (months of work, 10x capacity gain)
5. **Extract microservices** (quarters of work, team scaling gain)

---

## üîÆ Next Class Preview

**Hands-On Performance Lab:**
- Set up Redis caching for a Node.js app
- Implement database indexing strategies  
- Build a simple load balancer with Round Robin
- Measure performance improvements with real metrics
- Deploy to multiple servers and test scaling

**Come prepared to optimize!** ‚ö°

---

## üìö Additional Resources

### Caching and Performance
- [Redis Documentation](https://redis.io/documentation)
- [CDN Performance Guide](https://www.cloudflare.com/learning/cdn/)
- [Web Performance Best Practices](https://web.dev/performance/)

### Database Scaling  
- [Database Sharding Explained](https://www.mongodb.com/features/database-sharding-explained)
- [Replication Strategies](https://www.postgresql.org/docs/current/high-availability.html)
- [CAP Theorem Deep Dive](https://martin.kleppmann.io/2015/05/11/please-stop-calling-databases-cp-or-ap.html)

### Microservices Architecture
- [Building Microservices Book](https://martinfowler.com/books/microservices.html)
- [Shopify's Modular Monolith](https://shopify.engineering/shopify-monolith)
- [Netflix's Microservices Architecture](https://netflixtechblog.com/the-netflix-tech-blog-3f56c1e1b9fb)

### System Design Case Studies
- [High Scalability](http://highscalability.com/)
- [AWS Architecture Center](https://aws.amazon.com/architecture/)
- [System Design Interview Guide](https://github.com/donnemartin/system-design-primer)

---

*"Scalability is not about handling more users, it's about handling success."* üöÄ 